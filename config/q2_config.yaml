learning_rate: 0.001
batch_size: 32
layer_sizes: [21, 16, 1]
activations: ["relu", "linear"]
loss: "MSE"
epochs: 1000
l1_lambda: 0.0
l2_lambda: 0.0
